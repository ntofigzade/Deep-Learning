{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"Implements the RELU activation elementwise.\n",
    "\n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "\n",
    "    Returns:\n",
    "        A: Output of RELU function given Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    return A\n",
    "\n",
    "def relu_gradient(Z):\n",
    "    \"\"\"Computes the gradient of the RELU activation.\n",
    "\n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "        \n",
    "    Returns:\n",
    "        dz: Gradient of the RELU function with respect to Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    dz = np.zeros((Z.shape))\n",
    "    dz[Z > 0] = 1\n",
    "    \n",
    "    return dz\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Computes the softmax function for each column of Z.\n",
    "    \n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "    \n",
    "    Returns:\n",
    "        S: Output of the softmax function given Z.\n",
    "    \"\"\"\n",
    "    Z_exp = np.exp(Z)\n",
    "    Z_column_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "    S = Z_exp/Z_column_sum\n",
    "    \n",
    "    return S\n",
    "\n",
    "def one_hot_encoding(Y, C):\n",
    "    \"\"\"Implements one hot encoding to label vector.\n",
    "    \n",
    "    Args:\n",
    "        Y: Label vector of shape (number of classes, number of examples). \n",
    "        C: Number of labels to classify.\n",
    "        \n",
    "    Returns:\n",
    "        e: One hot encoding of label vector.\n",
    "    \"\"\"\n",
    "    e = np.zeros((Y.size, C))\n",
    "    e[np.arange(Y.size), Y] = 1\n",
    "    e = e.T\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\"Initializes weight and bias parameters for each network layer.\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: A list containing the size of each network layer.\n",
    "        \n",
    "    Returns:\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    params = dict()\n",
    "    L = len(layer_sizes) # number of layers including input layer\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(layer_sizes[l], layer_sizes[l - 1]) / np.sqrt(layer_sizes[l - 1])  # weight matrix\n",
    "        params['b' + str(l)] = np.zeros((layer_sizes[l], 1)) # bias vector\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forw_prop_l(W, A_prev, b, activation):\n",
    "    \"\"\"Implements the forward propagation computation for layer l.\n",
    "    \n",
    "    Args:\n",
    "        W: Weight matrix of current layer with dimension (size of current layer, size of previous layer).\n",
    "        A_prev: Activation matrix of previous layer with dimension (size of previous layer, number of examples) ).\n",
    "        b: Bias vector of current layer with dimension (size of current layer, 1).\n",
    "        activation: Activation function. \n",
    "    \n",
    "    Returns:\n",
    "        A: Output of the activation function.\n",
    "        cache: A tuple containing 'W', 'A_prev', 'b' and 'Z' to be used for the backward propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b # computes pre-activation parameter Z\n",
    "    A = activation(Z)\n",
    "    cache = (W, A_prev, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(params, X, activation_hidden, activation_output):\n",
    "    \"\"\"Implements the forward propagation computation for the network and outputs predictions vector.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of dimension (number of input features, number of examples)\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A: Output of the activation function of the output layer.\n",
    "        caches: A list of caches containing every cache of forw_prop_l function.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(params) // 2 # number of layers in the network excluding input layer\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A, cache = forw_prop_l(params['W' + str(l)], A, params['b' + str(l)], activation_output if l == L else activation_hidden)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(Y_hat, Y):\n",
    "    \"\"\"Implements the cross-entropy cost function.\n",
    "    \n",
    "    Args:\n",
    "        Y_hat: Probability vector corresponding to class predictions of shape (number of classes, number of examples).\n",
    "        Y: Ground truth vector of shape (number of classes, number of examples).\n",
    "        \n",
    "    Returns:\n",
    "        cost: Cross-entropy cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "    \n",
    "    cost = -np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    cost = np.squeeze(cost) # transform the cost from the shape of an array into the shape of a scalar\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backw_prop(cache, dA, activation_backward):\n",
    "    \"\"\"Implements the backward propagation computation for layer l.\n",
    "    \n",
    "    Args:\n",
    "        cache: The cache output (W, A_prev, b, Z) of forw_prop function called for current layer l.\n",
    "        dA: Gradient of the cost with respect to the activation of current layer l.\n",
    "        activation_backward: A function computing the gradient of activation function with respect to its input.\n",
    "        \n",
    "    Returns: \n",
    "        dA_prev: Gradient of the cost with respect to the activation of the previous layer.\n",
    "        dW: Gradient of the cost with respect to weight parameter of current layer l.\n",
    "        db: Gradient of the cost with respect to bias parameter of current layer l.\n",
    "    \"\"\"\n",
    "    \n",
    "    W, A_prev, b, Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dZ = dA * activation_backward(Z)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(caches, Y_hat, Y, hidden_activation_backward):\n",
    "    \"\"\"Implements the backward propagation computation for the network.\n",
    "    \n",
    "    Args:\n",
    "        caches: A list of caches which are the outputs of forw_prop functions for every layer l.\n",
    "        Y_hat: Probability vector corresponding to class predictions of shape (number of classes, number of examples).\n",
    "        Y: Ground truth matrix of shape (number of classes, number of examples).\n",
    "        \n",
    "    Returns:\n",
    "        grads: A dict mapping keys to the corresponding gradients of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = dict()\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches) # number of layers excluding input layer\n",
    "    Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # computes gradient of the cost with respect to Y_hat to initialize backward propagation.\n",
    "    W, A_prev, b, Z = caches[L - 1]\n",
    "    dZ = Y_hat - Y\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        cache_current = caches[l]\n",
    "        gradients = backw_prop(cache_current, grads[\"dA\" + str(l + 1)], hidden_activation_backward)\n",
    "        (grads[\"dA\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)]) = gradients\n",
    "                                                              \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, alpha):\n",
    "    \"\"\"Updates network parameters using gradient descent algorithm.\n",
    "    \n",
    "    Args:\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "        grads: A dict mapping keys to the corresponding gradients of the network.\n",
    "        alpha: Learning rate of the gradient descent algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        params: A dict mapping keys to the corresponding updated network parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(params) // 2 # number of layers excluding input layer\n",
    "    \n",
    "    for l in range(L):\n",
    "        params['W' + str(l + 1)] = params['W' + str(l + 1)] - alpha * grads['dW' + str(l + 1)]\n",
    "        params['b' + str(l+1)] = params['b' + str(l + 1)] - alpha * grads['db' + str(l + 1)]\n",
    "        \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    \"\"\" Flattens a multi-dimensional array except its first dimension corresponding to number of example.\"\"\" \n",
    "    \n",
    "    return data.reshape(data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = flatten(X_train) / 255\n",
    "X_test = flatten(X_test) / 255\n",
    "layer_sizes = [784, 10, 10, 20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, mini_batch_size = 128):\n",
    "    \"\"\"Creates a list of random minibatches from (X, Y).\n",
    "    \n",
    "    Args:\n",
    "        X: Input data of shape (number of input features, number of examples).\n",
    "        Y: Ground truth vector of shape (1, number of examples).\n",
    "        mini_batch_size: Size of mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "        mini_batches: A list of mini-batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # shuffles (X, Y)\n",
    "    permute = np.random.permutation(m)\n",
    "    X, Y = X[:, permute], Y[:, permute]\n",
    "    Y = Y\n",
    "    \n",
    "    # partition (X, Y)\n",
    "    n = m // mini_batch_size # number of mini batches of size mini_batch_size \n",
    "    for k in range(n):\n",
    "        mini_batch_X = X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    if m % mini_batch_size:\n",
    "        mini_batch_X = X[:, n * mini_batch_size :]\n",
    "        mini_batch_Y = Y[:, n * mini_batch_size :]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, params):\n",
    "    \"\"\"Implements forward propagation on a test set and makes predictions.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of shape (number of input features, number of examples)\n",
    "        Y: Ground truth vector of shape (1, number of examples)\n",
    "        params: Network parameters learnt by the model.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: An amount of correct predictions as percentage.\n",
    "        predicted: A vector of predicted labels of shape (1, number of examples).\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    A, caches = forward_propagation(params, X, relu, softmax)\n",
    "    \n",
    "    predicted = np.argmax(A, axis=0).reshape(Y.shape)\n",
    "    \n",
    "    accuracy = np.mean(predicted == Y)\n",
    "    \n",
    "    return accuracy, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, X_test, Y_test, layer_sizes, mini_batch_size = 32, alpha = 0.005, epochs = 20):\n",
    "    \"\"\"Implements a deep neural network.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of shape (number of input features, number of examples)\n",
    "        Y: Ground truth vector of shape (1, number of examples)\n",
    "        layer_sizes: A list containing the size of each network layer.\n",
    "        alpha: Learning rate of the gradient descent algorithm.\n",
    "        epochs: Number of passes through whole training data\n",
    "        \n",
    "    Returns:\n",
    "        params: Network parameters learnt by the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.T # stacks examples in columns\n",
    "    m = X.shape[1]\n",
    "    Y = Y.reshape(1, m)\n",
    "    costs = []\n",
    "    \n",
    "    params = init_params(layer_sizes)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        minibatches = mini_batches(X, Y, mini_batch_size)\n",
    "        total_cost = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            minibatch_X, minibatch_Y = minibatch\n",
    "            minibatch_Y = one_hot_encoding(minibatch_Y, 10)\n",
    "            \n",
    "            Y_hat, caches = forward_propagation(params, minibatch_X, relu, softmax)\n",
    "            total_cost += cross_entropy_cost(Y_hat, minibatch_Y)\n",
    "            grads = backward_propagation(caches, Y_hat, minibatch_Y, relu_gradient)\n",
    "            params = update_params(params, grads, alpha)\n",
    "        \n",
    "        cost = total_cost / m\n",
    "        accuracy, prediction = predict(X_test.T, Y_test.reshape(1, Y_test.size), params)\n",
    "        if epoch % 1 == 0:\n",
    "            print (\"Cost and Accuracy after iteration %i: %f, %a\" %(epoch, cost, accuracy))\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(alpha))\n",
    "    plt.show()\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost and Accuracy after iteration 0: 1.862017, 0.5833\n",
      "Cost and Accuracy after iteration 1: 0.825778, 0.8395\n",
      "Cost and Accuracy after iteration 2: 0.497034, 0.8715\n",
      "Cost and Accuracy after iteration 3: 0.425149, 0.8831\n",
      "Cost and Accuracy after iteration 4: 0.389593, 0.8927\n",
      "Cost and Accuracy after iteration 5: 0.365237, 0.8976\n",
      "Cost and Accuracy after iteration 6: 0.344245, 0.9033\n",
      "Cost and Accuracy after iteration 7: 0.326045, 0.9079\n",
      "Cost and Accuracy after iteration 8: 0.310653, 0.9107\n",
      "Cost and Accuracy after iteration 9: 0.297661, 0.9105\n",
      "Cost and Accuracy after iteration 10: 0.288059, 0.9165\n",
      "Cost and Accuracy after iteration 11: 0.279085, 0.9186\n",
      "Cost and Accuracy after iteration 12: 0.271073, 0.9165\n",
      "Cost and Accuracy after iteration 13: 0.265723, 0.9222\n",
      "Cost and Accuracy after iteration 14: 0.260562, 0.9228\n",
      "Cost and Accuracy after iteration 15: 0.255411, 0.9196\n",
      "Cost and Accuracy after iteration 16: 0.250755, 0.9227\n",
      "Cost and Accuracy after iteration 17: 0.246163, 0.9255\n",
      "Cost and Accuracy after iteration 18: 0.242854, 0.9296\n",
      "Cost and Accuracy after iteration 19: 0.239361, 0.9305\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = model(X_train, Y_train, X_test, Y_test, layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
