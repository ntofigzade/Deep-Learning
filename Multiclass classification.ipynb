{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"Implements the RELU activation elementwise.\n",
    "\n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "\n",
    "    Returns:\n",
    "        A: Output of RELU function given Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    return A\n",
    "\n",
    "def relu_gradient(Z):\n",
    "    \"\"\"Computes the gradient of the RELU activation.\n",
    "\n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "        \n",
    "    Returns:\n",
    "        dz: Gradient of the RELU function with respect to Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    dz = np.zeros((Z.shape))\n",
    "    dz[Z > 0] = 1\n",
    "    \n",
    "    return dz\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Computes the softmax function for each column of Z.\n",
    "    \n",
    "    Args:\n",
    "        Z: Numpy array of arbitrary shape.\n",
    "    \n",
    "    Returns:\n",
    "        S: Output of the softmax function given Z.\n",
    "    \"\"\"\n",
    "    Z_exp = np.exp(Z)\n",
    "    Z_column_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "    S = Z_exp/Z_column_sum\n",
    "    \n",
    "    return S\n",
    "\n",
    "def one_hot_encoding(Y, C):\n",
    "    \"\"\"Implements one hot encoding to label vector.\n",
    "    \n",
    "    Args:\n",
    "        Y: Label vector of shape (number of classes, number of examples). \n",
    "        C: Number of labels to classify.\n",
    "        \n",
    "    Returns:\n",
    "        e: One hot encoding of label vector.\n",
    "    \"\"\"\n",
    "    e = np.zeros((Y.size, C))\n",
    "    e[np.arange(Y.size), Y] = 1\n",
    "    e = e.T\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_sizes):\n",
    "    \"\"\"Initializes weight and bias parameters for each network layer.\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: A list containing the size of each network layer.\n",
    "        \n",
    "    Returns:\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    params = dict()\n",
    "    L = len(layer_sizes) # number of layers including input layer\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(layer_sizes[l], layer_sizes[l - 1]) / np.sqrt(layer_sizes[l - 1])  # weight matrix\n",
    "        params['b' + str(l)] = np.zeros((layer_sizes[l], 1)) # bias vector\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forw_prop_l(W, A_prev, b, activation):\n",
    "    \"\"\"Implements the forward propagation computation for layer l.\n",
    "    \n",
    "    Args:\n",
    "        W: Weight matrix of current layer with dimension (size of current layer, size of previous layer).\n",
    "        A_prev: Activation matrix of previous layer with dimension (size of previous layer, number of examples) ).\n",
    "        b: Bias vector of current layer with dimension (size of current layer, 1).\n",
    "        activation: Activation function. \n",
    "    \n",
    "    Returns:\n",
    "        A: Output of the activation function.\n",
    "        cache: A tuple containing 'W', 'A_prev', 'b' and 'Z' to be used for the backward propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b # computes pre-activation parameter Z\n",
    "    A = activation(Z)\n",
    "    cache = (W, A_prev, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(params, X, activation_hidden, activation_output):\n",
    "    \"\"\"Implements the forward propagation computation for the network and outputs predictions vector.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of dimension (number of input features, number of examples)\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A: Output of the activation function of the output layer.\n",
    "        caches: A list of caches containing every cache of forw_prop_l function.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(params) // 2 # number of layers in the network excluding input layer\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A, cache = forw_prop_l(params['W' + str(l)], A, params['b' + str(l)], activation_output if l == L else activation_hidden)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(Y_hat, Y):\n",
    "    \"\"\"Implements the cross-entropy cost function.\n",
    "    \n",
    "    Args:\n",
    "        Y_hat: Probability vector corresponding to class predictions of shape (number of classes, number of examples).\n",
    "        Y: Ground truth vector of shape (number of classes, number of examples).\n",
    "        \n",
    "    Returns:\n",
    "        cost: Cross-entropy cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "    \n",
    "    cost = -np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    cost = np.squeeze(cost) # transform the cost from the shape of an array into the shape of a scalar\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backw_prop(cache, dA, activation_backward):\n",
    "    \"\"\"Implements the backward propagation computation for layer l.\n",
    "    \n",
    "    Args:\n",
    "        cache: The cache output (W, A_prev, b, Z) of forw_prop function called for current layer l.\n",
    "        dA: Gradient of the cost with respect to the activation of current layer l.\n",
    "        activation_backward: A function computing the gradient of activation function with respect to its input.\n",
    "        \n",
    "    Returns: \n",
    "        dA_prev: Gradient of the cost with respect to the activation of the previous layer.\n",
    "        dW: Gradient of the cost with respect to weight parameter of current layer l.\n",
    "        db: Gradient of the cost with respect to bias parameter of current layer l.\n",
    "    \"\"\"\n",
    "    \n",
    "    W, A_prev, b, Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dZ = dA * activation_backward(Z)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(caches, Y_hat, Y, hidden_activation_backward):\n",
    "    \"\"\"Implements the backward propagation computation for the network.\n",
    "    \n",
    "    Args:\n",
    "        caches: A list of caches which are the outputs of forw_prop functions for every layer l.\n",
    "        Y_hat: Probability vector corresponding to class predictions of shape (number of classes, number of examples).\n",
    "        Y: Ground truth matrix of shape (number of classes, number of examples).\n",
    "        \n",
    "    Returns:\n",
    "        grads: A dict mapping keys to the corresponding gradients of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = dict()\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches) # number of layers excluding input layer\n",
    "    Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # computes gradient of the cost with respect to Y_hat to initialize backward propagation.\n",
    "    W, A_prev, b, Z = caches[L - 1]\n",
    "    dZ = Y_hat - Y\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        cache_current = caches[l]\n",
    "        gradients = backw_prop(cache_current, grads[\"dA\" + str(l + 1)], hidden_activation_backward)\n",
    "        (grads[\"dA\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)]) = gradients\n",
    "                                                              \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, alpha):\n",
    "    \"\"\"Updates network parameters using gradient descent algorithm.\n",
    "    \n",
    "    Args:\n",
    "        params: A dict mapping keys to the corresponding network parameters.\n",
    "        grads: A dict mapping keys to the corresponding gradients of the network.\n",
    "        alpha: Learning rate of the gradient descent algorithm.\n",
    "        \n",
    "    Returns:\n",
    "        params: A dict mapping keys to the corresponding updated network parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(params) // 2 # number of layers excluding input layer\n",
    "    \n",
    "    for l in range(L):\n",
    "        params['W' + str(l + 1)] = params['W' + str(l + 1)] - alpha * grads['dW' + str(l + 1)]\n",
    "        params['b' + str(l+1)] = params['b' + str(l + 1)] - alpha * grads['db' + str(l + 1)]\n",
    "        \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    \"\"\" Flattens a multi-dimensional array except its first dimension corresponding to number of example.\"\"\" \n",
    "    \n",
    "    return data.reshape(data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = flatten(X_train) / 255\n",
    "X_test = flatten(X_test) / 255\n",
    "layer_sizes = [784, 256, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, mini_batch_size = 128):\n",
    "    \"\"\"Creates a list of random minibatches from (X, Y).\n",
    "    \n",
    "    Args:\n",
    "        X: Input data of shape (number of input features, number of examples).\n",
    "        Y: Ground truth vector of shape (1, number of examples).\n",
    "        mini_batch_size: Size of mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "        mini_batches: A list of mini-batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # shuffles (X, Y)\n",
    "    permute = np.random.permutation(m)\n",
    "    X, Y = X[:, permute], Y[:, permute]\n",
    "    Y = Y\n",
    "    \n",
    "    # partition (X, Y)\n",
    "    n = m // mini_batch_size # number of mini batches of size mini_batch_size \n",
    "    for k in range(n):\n",
    "        mini_batch_X = X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    if m % mini_batch_size:\n",
    "        mini_batch_X = X[:, n * mini_batch_size :]\n",
    "        mini_batch_Y = Y[:, n * mini_batch_size :]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, params):\n",
    "    \"\"\"Implements forward propagation on a test set and makes predictions.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of shape (number of input features, number of examples)\n",
    "        Y: Ground truth vector of shape (1, number of examples)\n",
    "        params: Network parameters learnt by the model.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: An amount of correct predictions as percentage.\n",
    "        predicted: A vector of predicted labels of shape (1, number of examples).\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    A, caches = forward_propagation(params, X, relu, softmax)\n",
    "    \n",
    "    predicted = np.argmax(A, axis=0).reshape(Y.shape)\n",
    "    \n",
    "    accuracy = np.mean(predicted == Y)\n",
    "    \n",
    "    return accuracy, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, layer_sizes, mini_batch_size = 64, alpha = 0.01, epochs = 10):\n",
    "    \"\"\"Implements a deep neural network.\n",
    "    \n",
    "    Args:\n",
    "        X: Data of shape (number of input features, number of examples)\n",
    "        Y: Ground truth vector of shape (1, number of examples)\n",
    "        layer_sizes: A list containing the size of each network layer.\n",
    "        alpha: Learning rate of the gradient descent algorithm.\n",
    "        epochs: Number of passes through whole training data\n",
    "        \n",
    "    Returns:\n",
    "        params: Network parameters learnt by the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.T # stacks examples in columns\n",
    "    m = X.shape[1]\n",
    "    Y = Y.reshape(1, m)\n",
    "    costs = []\n",
    "    \n",
    "    params = init_params(layer_sizes)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        minibatches = mini_batches(X, Y, mini_batch_size)\n",
    "        total_cost = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            minibatch_X, minibatch_Y = minibatch\n",
    "            minibatch_Y = one_hot_encoding(minibatch_Y, 10)\n",
    "            \n",
    "            Y_hat, caches = forward_propagation(params, minibatch_X, relu, softmax)\n",
    "            total_cost += cross_entropy_cost(Y_hat, minibatch_Y)\n",
    "            grads = backward_propagation(caches, Y_hat, minibatch_Y, relu_gradient)\n",
    "            params = update_params(params, grads, alpha)\n",
    "        \n",
    "        cost = total_cost / m\n",
    "        accuracy, prediction = predict(X, Y, params)\n",
    "        if epoch % 1 == 0:\n",
    "            print (\"Cost and Accuracy after iteration %i: %f, %a\" %(epoch, cost, accuracy))\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(alpha))\n",
    "    plt.show()\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost and Accuracy after iteration 0: 0.959202, 0.87605\n",
      "Cost and Accuracy after iteration 1: 0.436222, 0.89545\n",
      "Cost and Accuracy after iteration 2: 0.363927, 0.9055833333333333\n",
      "Cost and Accuracy after iteration 3: 0.329493, 0.9110333333333334\n",
      "Cost and Accuracy after iteration 4: 0.306140, 0.9172333333333333\n",
      "Cost and Accuracy after iteration 5: 0.288342, 0.9223166666666667\n",
      "Cost and Accuracy after iteration 6: 0.273574, 0.9261833333333334\n",
      "Cost and Accuracy after iteration 7: 0.260640, 0.9296666666666666\n",
      "Cost and Accuracy after iteration 8: 0.248898, 0.9325166666666667\n",
      "Cost and Accuracy after iteration 9: 0.238337, 0.9356\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = model(X_train, Y_train, layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9352, array([[7, 2, 1, ..., 4, 5, 6]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_test.T, Y_test.reshape(1, Y_test.size), params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
